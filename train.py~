from data import *
from models import RagaDetector
from torch.utils.data import DataLoader
import multiprocessing
import progressbar
from tensorboardX import SummaryWriter

writer = SummaryWriter()
bptt = 500

def repackage_hidden(h):
    """Wraps hidden states in new Tensors, to detach them from their history."""
    if isinstance(h, torch.Tensor):
        return h.detach()
    else:
        return tuple(repackage_hidden(v) for v in h)


def time_split(source, i):
    seq_len = min(bptt, source.size(2) - 1 - i)
    if seq_len == bptt:
        data = source[:, :, i:i + seq_len]
        return data
    else:
        return None


torch.set_default_tensor_type('torch.FloatTensor')

if __name__ == '__main__':
    ds = RagaDataset('/home/sauhaarda/Dataset')
    train_loader = DataLoader(
        ds,
        batch_size=15,
        num_workers=multiprocessing.cpu_count(),
        shuffle=True,
        collate_fn=PadCollate())
    model = RagaDetector().cuda()
    criterion = torch.nn.CrossEntropyLoss().cuda()
    optimizer = torch.optim.Adadelta(model.parameters())
    
    n_iter = 0
    batch_num = 0
    for epoch in range(100):
        epoch_loss = 0
        epoch_loss_ctr = 0
        for song, label in train_loader:
            hidden = model.init_hidden(song.size(0))
            batch_loss = 0
            batch_loss_ctr = 0
            for batch, i in progressbar.progressbar(enumerate(range(0, song.size(2) - 1, bptt))):
                hidden = repackage_hidden(hidden)
                x = time_split(song, i)
                if x is not None:
                    optimizer.zero_grad()
                    x, hidden = model(x.cuda(), hidden)
                    loss = criterion(x[-1], label.cuda())
                    loss.backward()
                    optimizer.step()
                    writer.add_scalar('data/noisy_loss', loss.item(), n_iter)
                    batch_loss += loss.item()
                    epoch_loss += loss.item()
                    n_iter += 1
                    batch_loss_ctr += 1.
                    epoch_loss_ctr += 1
                else:
                    break
            writer.add_scalar('data/batch_loss', batch_loss/ batch_loss_ctr, batch_num)
            for name, param in model.named_parameters():
                writer.add_histogram(name, param.clone().cpu().data.numpy(), batch_num)
            batch_num += 1
        writer.add_scalar('data/epoch_loss', epoch_loss/ epoch_loss_ctr, epoch)
